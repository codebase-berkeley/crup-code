{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c496f1b",
   "metadata": {},
   "source": [
    "## Classical ML Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae22d0a",
   "metadata": {},
   "source": [
    "#### To save your changes and use this notebook, choose File -> Save a copy in Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a1acc",
   "metadata": {},
   "source": [
    "This notebook contains a series of coding problems based on the \"Classical ML\" lecture. Each problem includes:\n",
    "1.  **Key Concepts:** A markdown-based summary of the relevant lecture slides.\n",
    "2.  **Problem:** A task to implement the concept in Python.\n",
    "3.  **Solution:** A code cell for you to fill in and a markdown cell for your analysis.\n",
    "\n",
    "We will use `numpy` for numerical operations, `matplotlib` for plotting, and `scikit-learn` for our ML models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to install libraries\n",
    "!pip install numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c944e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1a31b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Linear Regression\n",
    "\n",
    "### ðŸ§  Key Concepts from Lecture\n",
    "\n",
    "* **Machine Learning (ML):** Can be thought of as software systems that improve (learn) through data.\n",
    "* **Basic Recipe:** Use examples (data) to teach (fit) a model, then use the model to make decisions or predictions.\n",
    "* **Linear Regression:** A machine learning model where predictions are a linear function of the parameters.\n",
    "* **Model:** The model (or hypothesis) for linear regression is $\\hat{y} = w^T x + b$.\n",
    "* **Objective (Least Squares):** We want to find the parameters $w$ that minimize the error between our predictions and the true labels $y$. This is commonly done by minimizing the squared error: $min_{w} ||Xw - y||_2^2$.\n",
    "* **Solution:** The solution that minimizes this error is $w^* = (X^T X)^{-1} X^T y$.\n",
    "\n",
    "### âœï¸ Problem 1\n",
    "\n",
    "1.  Use `sklearn.datasets.make_regression` to create a simple toy dataset.\n",
    "2.  Split the data into training and testing sets.\n",
    "3.  Import and fit a `sklearn.linear_model.LinearRegression` model to the **training data**.\n",
    "4.  Create a scatter plot of **all** data points (train and test).\n",
    "5.  Plot the fitted regression line (from your model's predictions on the test set) over the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146db30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 1. Create a toy dataset\n",
    "X_lr, y_lr = make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n",
    "\n",
    "# 2. Split the data\n",
    "X_lr_train, X_lr_test, y_lr_train, y_lr_test = train_test_split(X_lr, y_lr, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Import and fit the model\n",
    "lr_model = ... # [TODO: Initialize the LinearRegression model]\n",
    "lr_model.fit(..., ...) # [TODO: Fit the model to the training data]\n",
    "\n",
    "print(f\"Model Intercept (b): {lr_model.intercept_}\")\n",
    "print(f\"Model Coefficient (w): {lr_model.coef_}\")\n",
    "\n",
    "# 4. Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_lr_train, y_lr_train, label='Training Data', alpha=0.7)\n",
    "plt.scatter(X_lr_test, y_lr_test, label='Test Data', alpha=0.7)\n",
    "\n",
    "# 5. Plot the fitted regression line\n",
    "# We sort the test data just to make the line plot continuous\n",
    "sorted_indices = np.argsort(X_lr_test.ravel())\n",
    "X_lr_test_sorted = X_lr_test[sorted_indices]\n",
    "y_lr_pred_sorted = ... # [TODO: Get predictions from the fitted model for the sorted test X]\n",
    "\n",
    "plt.plot(X_lr_test_sorted, y_lr_pred_sorted, color='red', linewidth=3, label='Fitted Regression Line') # [TODO: Plot the fitted regression line]\n",
    "\n",
    "plt.title('Linear Regression')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2f262",
   "metadata": {},
   "source": [
    "### ðŸ”¬ Analysis 1\n",
    "\n",
    "[TODO: Write your analysis here... What does the plot show? Does the line seem to be a good fit for the data?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5194ff42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Ridge Regression (L2 Regularization)\n",
    "\n",
    "### ðŸ§  Key Concepts from Lecture\n",
    "\n",
    "* **Overfitting:** A common problem where a model learns the training data *too* well, including its noise. This leads to poor performance on new, unseen test cases. This often happens when the model is too complex, for example, having too many features.\n",
    "* **Regularization:** A technique to \"tighten up\" the model by adding constraints, which helps prevent overfitting.\n",
    "* **L2-Regularization (Ridge Regression):** We add a penalty term to the loss function that penalizes large coefficient values ($w$).\n",
    "* **Loss Function:** $\\mathcal{L} = ||Xw - y||_2^2 + \\lambda ||w||_2^2$.\n",
    "* **The $\\lambda$ Hyperparameter:** This value (called `alpha` in scikit-learn) controls the **strength of the penalty**.\n",
    "    * A small $\\lambda$ ($\\lambda \\to 0$) is the same as ordinary least squares.\n",
    "    * A large $\\lambda$ ($\\lambda \\to \\infty$) will force the coefficients $w$ to shrink, approaching zero. This results in a simpler, \"stiffer\" model that is less likely to overfit.\n",
    "\n",
    "### âœï¸ Problem 2\n",
    "\n",
    "We will manually create a situation where linear regression overfits and then use Ridge to fix it.\n",
    "\n",
    "1.  Generate non-linear data (e.g., `y = np.sin(X)` with some noise).\n",
    "2.  Use `sklearn.preprocessing.PolynomialFeatures` (e.g., `degree=10`) to create a high-dimensional feature set. This will force our model to overfit.\n",
    "3.  Create a `for` loop to iterate through a list of $\\lambda$ (`alpha`) values: `[0.001, 1, 100]`.\n",
    "4.  Inside the loop:\n",
    "    * Fit a `sklearn.linear_model.Ridge` model with the current `alpha` value to the polynomial features.\n",
    "    * Plot the original data as a scatter plot.\n",
    "    * Plot the model's predictions to show the fitted curve.\n",
    "5.  Observe how the shape of the fitted curve changes as $\\lambda$ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# 1. Generate non-linear data\n",
    "np.random.seed(0)\n",
    "X_ridge = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y_ridge = np.sin(X_ridge).ravel() + np.random.normal(0, 0.5, 100)\n",
    "\n",
    "# 2. Set up polynomial features\n",
    "degree = 10 # This high degree will cause overfitting\n",
    "\n",
    "# 3. Loop through lambda values\n",
    "lambdas = [0.001, 1, 100]\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, l in enumerate(lambdas):\n",
    "    plt.subplot(1, len(lambdas), i + 1)\n",
    "    \n",
    "    # 4. Create and fit the model using a pipeline\n",
    "    # A pipeline simplifies steps: 1) create polynomial features, 2) scale, 3) fit Ridge model\n",
    "    ridge_model = make_pipeline(\n",
    "        PolynomialFeatures(degree), \n",
    "        StandardScaler(), \n",
    "        Ridge(alpha=l) \n",
    "    )\n",
    "    \n",
    "    ridge_model.fit(..., ...) # [TODO: Fit the pipeline to the data (X_ridge, y_ridge)]\n",
    "    \n",
    "    # Plot original data\n",
    "    plt.scatter(X_ridge, y_ridge, label='Data')\n",
    "    \n",
    "    # Plot model predictions\n",
    "    y_ridge_pred = ... # [TODO: Get predictions from the pipeline]\n",
    "    plt.plot(X_ridge, y_ridge_pred, color='red', linewidth=3, label='Fitted Model') # [TODO: Plot the fitted model's predictions]\n",
    "    \n",
    "    plt.title(f'Ridge Regression (Degree={degree})\\nLambda (alpha) = {l}')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04e5896",
   "metadata": {},
   "source": [
    "### ðŸ”¬ Analysis 2\n",
    "\n",
    "[TODO: Write your analysis here... How does the fitted curve change as Lambda (alpha) increases? What does this tell you about the bias-variance tradeoff?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5677c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. LASSO Regression (L1 Regularization)\n",
    "\n",
    "### ðŸ§  Key Concepts from Lecture\n",
    "\n",
    "* **L1-Regularization (LASSO):** We add a penalty term based on the *absolute value* of the coefficients.\n",
    "* **Loss Function:** $\\mathcal{L} = ||Xw - y||_2^2 + \\lambda ||w||_1$.\n",
    "* **Sparsity:** The key difference from Ridge. The L1 penalty is special because it **induces sparsity**. This means it can force many of the model's coefficients ($w$) to be **exactly zero**.\n",
    "* **Effect:** LASSO performs automatic *feature selection*. If a feature's coefficient becomes 0, it has been \"selected out\" of the model.\n",
    "* **The $\\lambda$ Hyperparameter:** Just like in Ridge, $\\lambda$ controls the penalty strength. A larger $\\lambda$ will result in a *more sparse* model (more zero coefficients).\n",
    "\n",
    "### âœï¸ Problem 3\n",
    "\n",
    "1.  Use `sklearn.datasets.make_regression` to create a dataset with `n_features=10` but only `n_informative=2`. This means 8 of the 10 features are irrelevant.\n",
    "2.  Create a `for` loop to iterate through a list of $\\lambda$ (`alpha`) values: `[0.1, 1, 10]`.\n",
    "3.  Inside the loop:\n",
    "    * Fit a `sklearn.linear_model.Lasso` model with the current `alpha`.\n",
    "    * Print the `alpha` value and the model's coefficients (`.coef_`).\n",
    "4.  Observe how many coefficients become zero as $\\lambda$ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac918a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# 1. Create a dataset with mostly uninformative features\n",
    "X_lasso, y_lasso, true_coef = make_regression(\n",
    "    n_samples=100, \n",
    "    n_features=10, \n",
    "    n_informative=2, # Only 2 features actually matter\n",
    "    noise=10,\n",
    "    coef=True, # Return the true coefficients\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "# Standardize features for regularization\n",
    "scaler = StandardScaler()\n",
    "X_lasso_scaled = scaler.fit_transform(X_lasso)\n",
    "\n",
    "print(\"--- True Coefficients (from data generator) ---\")\n",
    "print(np.round(true_coef, 2))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2. Loop through lambda values\n",
    "lambdas = [0.1, 1, 100]\n",
    "print(\"\\n--- LASSO Model Coefficients ---\")\n",
    "\n",
    "for l in lambdas:\n",
    "    # 3. Fit the model\n",
    "    lasso_model = ... # [TODO: Initialize the Lasso model with alpha=l]\n",
    "    ... # [TODO: Fit the model to the scaled training data]\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Alpha (Lambda) = {l}\")\n",
    "    print(np.round(lasso_model.coef_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f57ae8",
   "metadata": {},
   "source": [
    "### ðŸ”¬ Analysis 3\n",
    "\n",
    "[TODO: Write your analysis here... Compare the 'True Coefficients' to the coefficients from your LASSO models. What happens to the coefficients as alpha increases? Why is this useful?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a455c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Logistic Regression\n",
    "\n",
    "### ðŸ§  Key Concepts from Lecture\n",
    "\n",
    "* **Classification:** In contrast to regression (where the output is a real number), classification problems have an output that is a discrete label from a finite set, e.g., `{0, 1}`.\n",
    "* **Goal:** Learn a mapping $f_\\theta: X \\to \\{0, ..., K-1\\}$.\n",
    "* **Logistic Regression:** A classification method that models the *probability* of a class given an input, $p(y|x)$.\n",
    "* **The Logistic (Sigmoid) Function:** To model $p(y=1|x)$, it passes a linear function ($w^T x + b$) through the sigmoid function, which squashes any real-valued number into the range $[0, 1]$.\n",
    "    * $p(y=1|x) = \\frac{1}{1 + e^{-(w^T x + b)}}$\n",
    "* **Inference:** To make a final decision, we use a threshold (e.g., 0.5). If $p(y=1|x) > 0.5$, predict \"1\", otherwise predict \"0\".\n",
    "\n",
    "### âœï¸ Problem 4\n",
    "\n",
    "1.  **Part A: Explore the Sigmoid Function**\n",
    "    * Write a Python function `sigmoid(z)`.\n",
    "    * Create an array `z` of values from -10 to 10.\n",
    "    * Plot `z` vs. `sigmoid(z)`.\n",
    "    * Add a horizontal line at `y=0.5` and a vertical line at `x=0` to show the threshold.\n",
    "2.  **Part B: Apply Logistic Regression**\n",
    "    * Use `sklearn.datasets.make_classification` to create a 2-class dataset.\n",
    "    * Fit a `sklearn.linear_model.LogisticRegression` model.\n",
    "    * Visualize the results. This is more complex: you'll need to create a `meshgrid` to plot the decision boundary (the line where the model's output probability is 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d31767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Suppress a known warning from matplotlib's contourf\n",
    "warnings.filterwarnings('ignore', message='No contour levels were found')\n",
    "\n",
    "# --- Part A: Explore the Sigmoid Function ---\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Computes the sigmoid function.\"\"\"\n",
    "    return ... # [TODO: Implement the sigmoid function: 1 / (1 + exp(-z))]\n",
    "\n",
    "# Create z data\n",
    "z = np.linspace(-10, 10, 100)\n",
    "s_z = sigmoid(z)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(z, s_z, label='Sigmoid Function')\n",
    "plt.axhline(0.5, color='red', linestyle='--', label='Threshold (0.5)')\n",
    "plt.axvline(0, color='grey', linestyle='--', alpha=0.5)\n",
    "plt.title('The Logistic (Sigmoid) Function')\n",
    "plt.xlabel('z (linear input, wT*x + b)')\n",
    "plt.ylabel('Probability (p(y=1|x))')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "# --- Part B: Apply Logistic Regression ---\n",
    "\n",
    "# 1. Create dataset\n",
    "X_log, y_log = make_classification(\n",
    "    n_samples=100,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=2\n",
    ")\n",
    "\n",
    "# 2. Fit the model\n",
    "log_model = ... # [TODO: Initialize the LogisticRegression model]\n",
    "log_model.fit(..., ...) # [TODO: Fit the model to the data (X_log, y_log)]\n",
    "\n",
    "# 3. Visualize the decision boundary\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Create a meshgrid\n",
    "x_min, x_max = X_log[:, 0].min() - 1, X_log[:, 0].max() + 1\n",
    "y_min, y_max = X_log[:, 1].min() - 1, X_log[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "# Get predictions (probabilities) for the entire grid\n",
    "Z = log_model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1] # [TODO: Get the class 1 probabilities for the grid (np.c_[xx.ravel(), yy.ravel()])]\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary (where p=0.5) and probability regions\n",
    "plt.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.4, levels=[0, 0.5, 1])\n",
    "plt.contour(xx, yy, Z, colors='black', levels=[0.5], linestyles='--') # [TODO: Plot the decision boundary line where Z=0.5]\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X_log[:, 0], X_log[:, 1], c=y_log, cmap='coolwarm', edgecolors='k')\n",
    "plt.title('Logistic Regression Decision Boundary')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb25691",
   "metadata": {},
   "source": [
    "### ðŸ”¬ Analysis 4\n",
    "\n",
    "[SOLUTION CELL]\n",
    "\n",
    "[TODO: Write your analysis here... What does the sigmoid function plot show? What does the decision boundary plot show? What is the relationship between them?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087a00d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Model Evaluation: ROC and Precision-Recall Curves\n",
    "\n",
    "### ðŸ§  Key Concepts from Lecture\n",
    "\n",
    "* **Evaluating Classifiers:** We need metrics to decide which model is better.\n",
    "* **Confusion Matrix:** Once we set a decision threshold (e.g., 0.5), we can categorize all predictions:\n",
    "    * **True Positive (TP):** Actual is 1, Predicted is 1.\n",
    "    * **True Negative (TN):** Actual is -1, Predicted is -1.\n",
    "    * **False Positive (FP):** Actual is -1, Predicted is 1.\n",
    "    * **False Negative (FN):** Actual is 1, Predicted is -1.\n",
    "* **Key Rates:**\n",
    "    * **True Positive Rate (TPR) / Recall:** $TPR = \\frac{TP}{\\text{Actual Positives}} = \\frac{TP}{TP + FN}$. Also called Sensitivity.\n",
    "    * **False Positive Rate (FPR):** $FPR = \\frac{FP}{\\text{Actual Negatives}} = \\frac{FP}{FP + TN}$.\n",
    "    * **Precision:** $\\text{Precision} = \\frac{TP}{\\text{Predicted Positives}} = \\frac{TP}{TP + FP}$.\n",
    "* **ROC (Receiver Operating Characteristic) Curve:**\n",
    "    * Plots **TPR (Y-axis) vs. FPR (X-axis)**.\n",
    "    * It's generated by varying the decision threshold from 0 to 1.\n",
    "    * A **better** classifier's curve \"goes toward the top left\".\n",
    "    * The diagonal line represents a random classifier.\n",
    "    * **AUC (Area Under the Curve):** A single number summary. The bigger the AUC, the better the model. An AUC of 1.0 is perfect, and 0.5 is random.\n",
    "* **Precision-Recall (PR) Curve:**\n",
    "    * Plots **Precision (Y-axis) vs. Recall (X-axis)**.\n",
    "    * Useful when you care about the performance on the *positive* class, especially if the classes are imbalanced.\n",
    "\n",
    "### âœï¸ Problem 5\n",
    "\n",
    "You are given the true labels and the raw probability scores from two different models, Model A and Model B.\n",
    "\n",
    "1.  Use the provided `y_true`, `y_scores_A`, and `y_scores_B`.\n",
    "2.  **ROC Analysis:**\n",
    "    * Use `sklearn.metrics.roc_curve` to get the FPR and TPR for both models.\n",
    "    * Use `sklearn.metrics.auc` to calculate the AUC for both.\n",
    "    * Plot both ROC curves on the same graph. Include the \"random classifier\" dashed line.\n",
    "    * Add the AUC scores to the legend.\n",
    "3.  **PR Analysis:**\n",
    "    * Use `sklearn.metrics.precision_recall_curve` to get the precision and recall for both models.\n",
    "    * Plot both PR curves on the same graph.\n",
    "4.  Conclude which model is better and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7994605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Generate toy data: 128 points\n",
    "n_pos = 38\n",
    "n_neg = 90\n",
    "y_true = np.array([1]*n_pos + [0]*n_neg)\n",
    "\n",
    "# Model A\n",
    "scores_A_pos = np.random.beta(a=1, b=2, size=n_pos)\n",
    "scores_A_neg = np.random.beta(a=1, b=12, size=n_neg)\n",
    "y_scores_A = np.concatenate([scores_A_pos, scores_A_neg])\n",
    "\n",
    "# Model B\n",
    "scores_B_pos = np.random.beta(a=12, b=1, size=n_pos)\n",
    "scores_B_neg = np.random.beta(a=4, b=3, size=n_neg)\n",
    "y_scores_B = np.concatenate([scores_B_pos, scores_B_neg])\n",
    "\n",
    "# Model C\n",
    "scores_C_pos = np.random.beta(a=2, b=2, size=n_pos)\n",
    "scores_C_neg = np.random.beta(a=2, b=3, size=n_neg)\n",
    "y_scores_C = np.concatenate([scores_C_pos, scores_C_neg])\n",
    "\n",
    "# Model D\n",
    "scores_D_pos = np.random.beta(a=15, b=1, size=n_pos)\n",
    "scores_D_neg = np.random.beta(a=1, b=15, size=n_neg)\n",
    "y_scores_D = np.concatenate([scores_D_pos, scores_D_neg])\n",
    "\n",
    "idx = np.arange(y_true.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "y_true = y_true[idx]\n",
    "y_scores_A = y_scores_A[idx]\n",
    "y_scores_B = y_scores_B[idx]\n",
    "y_scores_C = y_scores_C[idx]\n",
    "y_scores_D = y_scores_D[idx]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# --- 2. ROC Analysis ---\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# Model A\n",
    "fpr_A, tpr_A, _ = ... # [TODO: Calculate ROC curve for Model A]\n",
    "roc_auc_A = ... # [TODO: Calculate AUC for Model A]\n",
    "\n",
    "# Model B\n",
    "fpr_B, tpr_B, _ = ... # [TODO: Calculate ROC curve for Model B]\n",
    "roc_auc_B = ... # [TODO: Calculate AUC for Model B]\n",
    "\n",
    "# Model C\n",
    "fpr_C, tpr_C, _ = ... # [TODO: Calculate ROC curve for Model C]\n",
    "roc_auc_C = ... # [TODO: Calculate AUC for Model C]\n",
    "\n",
    "# Model D\n",
    "fpr_D, tpr_D, _ = ... # [TODO: Calculate ROC curve for Model D]\n",
    "roc_auc_D = ... # [TODO: Calculate AUC for Model D]\n",
    "\n",
    "# Plotting\n",
    "plt.plot(fpr_A, tpr_A, label=f'Model A (AUC = {roc_auc_A:.2f})')\n",
    "... # [TODO: Plot Model B's ROC curve]\n",
    "... # [TODO: Plot Model C's ROC curve]\n",
    "... # [TODO: Plot Model D's ROC curve]\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Classifier') \n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate (FPR)') \n",
    "plt.ylabel('True Positive Rate (TPR) / Recall') \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# --- 3. PR Analysis ---\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Model A\n",
    "prec_A, rec_A, _ = ... # [TODO: Calculate PR curve for Model A]\n",
    "\n",
    "# Model B\n",
    "prec_B, rec_B, _ = ... # [TODO: Calculate PR curve for Model B]\n",
    "\n",
    "# Model C\n",
    "prec_C, rec_C, _ = ... # [TODO: Calculate PR curve for Model C]\n",
    "\n",
    "# Model D\n",
    "prec_D, rec_D, _ = ... # [TODO: Calculate PR curve for Model D]\n",
    "\n",
    "# Plotting\n",
    "plt.plot(rec_A, prec_A, label='Model A')\n",
    "... # [TODO: Plot Model B's PR curve]\n",
    "... # [TODO: Plot Model C's PR curve]\n",
    "... # [TODO: Plot Model D's PR curve]\n",
    "plt.plot([0, 1], [0.5, 0.5], color='red', linestyle='--', label='Random Classifier')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall (TPR)') \n",
    "plt.ylabel('Precision') \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f54bdd6",
   "metadata": {},
   "source": [
    "### ðŸ”¬ Analysis 5\n",
    "\n",
    "[TODO: Write your analysis here... Based on both the ROC/AUC and the PR curves, which model is better? Why?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c343a2d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. K-Means Clustering\n",
    "\n",
    "### ðŸ§  Key Concepts from Lecture\n",
    "\n",
    "* **Unsupervised Learning:** In contrast to supervised learning, we are given data without labels, $\\{x_i\\}_{i=1}^N$.\n",
    "* **Goal:** Discover structure in the unlabeled data.\n",
    "* **Clustering:** A common unsupervised task where we assign each data point $x_i$ to a cluster label $z_i \\in \\{1, ..., K\\}$.\n",
    "* **K-Means:** A popular centroid-based clustering algorithm.\n",
    "    * It represents each of the $K$ clusters by a single point, its **centroid** $c_k$.\n",
    "    * **Optimization Problem:** It tries to find the cluster assignments and centroids that minimize the total within-cluster sum of squares (WCSS), i.e., the sum of squared distances from each point to its assigned centroid: $argmin \\sum_k \\sum_{x \\in C_k} ||x - c_k||_2^2$.\n",
    "* **The 'K' Hyperparameter:** $K$ is the **number of clusters**. It is a hyperparameter that *you* must provide to the algorithm.\n",
    "    * Choosing the wrong $K$ can lead to bad results (lumping distinct groups or splitting natural ones).\n",
    "    * **Elbow Method:** A common way to choose $K$ is to run K-Means for a range of $K$ values, plot the WCSS (called `inertia_` in scikit-learn) for each $K$, and look for an \"Elbow Point\". This point represents a good tradeoff between model simplicity and minimizing error.\n",
    "\n",
    "### âœï¸ Problem 6\n",
    "\n",
    "1.  Use `sklearn.datasets.make_blobs` to create a dataset with 4 distinct clusters (`centers=4`).\n",
    "2.  **Part A: Apply K-Means**\n",
    "    * Fit a `sklearn.cluster.KMeans` model with `n_clusters=4`.\n",
    "    * Create a scatter plot of the data, colored by the *labels* returned from the fitted model (`.labels_`).\n",
    "    * Plot the final cluster centroids (`.cluster_centers_`) on top as large 'X's or circles.\n",
    "3.  **Part B: Find the best K (Elbow Method)**\n",
    "    * Create an empty list `wcss` (or `inertia`).\n",
    "    * Loop `k` from 1 to 10.\n",
    "    * Inside the loop, fit a `KMeans` model with `n_clusters=k` and append its `.inertia_` to your list.\n",
    "    * Plot the `k` values (1-10) vs. the `wcss` list.\n",
    "    * Identify the \"elbow.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ebbe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 1. Create dataset\n",
    "X_km, y_true_km = make_blobs(\n",
    "    n_samples=300,\n",
    "    centers=4, # The true number of clusters\n",
    "    cluster_std=1.0,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# --- 2. Part A: Apply K-Means ---\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# Fit K-Means\n",
    "kmeans_model = ... # [TODO: Initialize KMeans with n_clusters=4, random_state=0, and n_init=10]\n",
    "... # [TODO: Fit the model to the data X_km]\n",
    "\n",
    "# Get labels and centroids\n",
    "labels = ... # [TODO: Get the cluster labels from the model]\n",
    "centroids = ... # [TODO: Get the cluster centroids from the model]\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X_km[:, 0], X_km[:, 1], c=labels, cmap='viridis', alpha=0.7, label='Data Points')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=200, color='red', label='Centroids') # [TODO: Plot the centroids]\n",
    "plt.title('6a. K-Means Clustering (K=4)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# --- 3. Part B: Find the best K (Elbow Method) ---\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "wcss = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans_k = ... # [TODO: Initialize KMeans with n_clusters=k]\n",
    "    ... # [TODO: Fit the model to X_km]\n",
    "    wcss.append(...) # [TODO: Append the model's inertia (WCSS) to the list]\n",
    "\n",
    "# Plot\n",
    "plt.plot(k_range, wcss, marker='o')\n",
    "plt.title('6b. The Elbow Method for finding K')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('WCSS (Inertia)')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.axvline(4, color='red', linestyle='--', label='Elbow Point (K=4)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e09b8",
   "metadata": {},
   "source": [
    "### ðŸ”¬ Analysis 6\n",
    "\n",
    "[TODO: Write your analysis here... Does the K-Means plot (6a) look like it found the correct clusters? What does the elbow plot (6b) tell you? Where is the 'elbow' and what does it mean?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af55637",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Random Forests & Gradient Boosting\n",
    "\n",
    "### ðŸ§  Key Concepts from Lecture\n",
    "\n",
    "* **Decision Tree:** A tree-like model that makes predictions by recursively splitting data based on feature values. Each node is a question on a feature, each branch is an outcome, and each leaf is a final prediction.\n",
    "* **Ensembling:** The process of combining multiple \"weak learner\" models to create a single, strong, and accurate predictor. The intuition is that their combined strengths reinforce each other, while their individual errors cancel out.\n",
    "* **Random Forest:** A powerful ensemble model that is:\n",
    "    1.  An **ensemble of many decision trees**.\n",
    "    2.  Trained using **Bagging:** Each tree is trained on a random \"bootstrap sample\" (sample with replacement) of the data.\n",
    "    3.  Uses **Random Features:** At each split in a tree, it only considers a random subset of features.\n",
    "* **Result:** This combination makes Random Forests highly accurate, robust to noise, and less likely to overfit than a single decision tree. It can capture complex, non-linear patterns.\n",
    "* **Gradient Boosting:** (Not in the slides, but requested). This is another, even more powerful, ensemble technique. Instead of building independent trees (like Random Forest), it builds trees *sequentially*, where each new tree tries to correct the errors made by the previous ones. It is often a top-performing model in industry.\n",
    "\n",
    "### âœï¸ Problem 7\n",
    "\n",
    "Linear models fail on non-linear data. Tree ensembles excel here.\n",
    "1.  Use `sklearn.datasets.make_moons` to create a non-linear dataset.\n",
    "2.  Fit a `sklearn.ensemble.RandomForestClassifier`.\n",
    "3.  Fit a `sklearn.ensemble.GradientBoostingClassifier`.\n",
    "4.  Plot the decision boundaries for both models (using the `meshgrid` technique from Problem 4) to show how they capture the non-linear \"moon\" shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe896e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# 1. Create non-linear \"moons\" data\n",
    "X_tree, y_tree = make_moons(n_samples=200, noise=0.3, random_state=0)\n",
    "\n",
    "# Split data\n",
    "X_tree_train, X_tree_test, y_tree_train, y_tree_test = train_test_split(X_tree, y_tree, test_size=0.3, random_state=42)\n",
    "\n",
    "# 2. Fit Random Forest\n",
    "rf_model = ... # [TODO: Initialize RandomForestClassifier(n_estimators=100, random_state=0)]\n",
    "rf_model.fit(..., ...) # [TODO: Fit the model to the training data]\n",
    "\n",
    "# 3. Fit Gradient Boosting\n",
    "gb_model = ... # [TODO: Initialize GradientBoostingClassifier(n_estimators=100, random_state=0)]\n",
    "gb_model.fit(..., ...) # [TODO: Fit the model to the training data]\n",
    "\n",
    "# 4. Plot decision boundaries\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Create meshgrid\n",
    "x_min, x_max = X_tree[:, 0].min() - 1, X_tree[:, 0].max() + 1\n",
    "y_min, y_max = X_tree[:, 1].min() - 1, X_tree[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "# Plot for Random Forest\n",
    "plt.subplot(1, 2, 1)\n",
    "Z_rf = rf_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_rf = Z_rf.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z_rf, cmap='coolwarm', alpha=0.4)\n",
    "plt.scatter(X_tree_test[:, 0], X_tree_test[:, 1], c=y_tree_test, cmap='coolwarm', edgecolors='k')\n",
    "plt.title('Random Forest Decision Boundary')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Plot for Gradient Boosting\n",
    "plt.subplot(1, 2, 2)\n",
    "Z_gb = gb_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_gb = Z_gb.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z_gb, cmap='coolwarm', alpha=0.4)\n",
    "plt.scatter(X_tree_test[:, 0], X_tree_test[:, 1], c=y_tree_test, cmap='coolwarm', edgecolors='k')\n",
    "plt.title('Gradient Boosting Decision Boundary')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c1485",
   "metadata": {},
   "source": [
    "### ðŸ”¬ Analysis 7\n",
    "\n",
    "[TODO: Write your analysis here... How do these decision boundaries differ from the Logistic Regression one? Why are these models able to capture the 'moon' shape?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272167ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Principal Component Analysis (PCA)\n",
    "\n",
    "### ðŸ§  Key Concepts from Lecture\n",
    "\n",
    "* **Dimensionality Reduction:** An unsupervised learning task where we want to map data from a high-dimensional space ($R^d$) to a lower-dimensional one ($R^m$), where $m \\ll d$.\n",
    "* **Manifold Hypothesis:** The idea that high-dimensional data often tends to lie near a much lower-dimensional manifold (a space that locally feels Euclidean). For example, a 3D \"swiss roll\" can be \"unrolled\" into 2D without losing information.\n",
    "* **Principal Components Analysis (PCA):** A popular dimensionality reduction technique.\n",
    "    * **Goal:** It finds the \"directions\" (principal components) in the original space that retain the most \"information\".\n",
    "    * **Mechanism:** These directions are the ones that have the **maximal variance** in the data. The 1st component is the direction of max variance. The 2nd component is the direction of next-highest variance, *and* is orthogonal (perpendicular) to the first.\n",
    "* **Hyperparameter 'K':** K (or `n_components` in scikit-learn) is the number of principal components you choose to keep.\n",
    "* **Choosing K (% Variance Explained):** A common way to pick K is to look at the **cumulative \"percent variance explained\"**. The normalized eigenvalues of the covariance matrix tell you how much variance each component \"explains\". You can plot this and pick a K that explains, for example, 90% or 95% of the total variance.\n",
    "\n",
    "### âœï¸ Problem 8\n",
    "\n",
    "We will use the `sklearn.datasets.fetch_olivetti_faces` dataset, a classic example for PCA (similar to the one in the lecture). Each image is 64x64 pixels, giving us **4096 dimensions**. We want to see if we can reduce this high-dimensional data.\n",
    "\n",
    "1.  Load the faces dataset and its data `X` and `y` targets.\n",
    "2.  Standardize the data using `StandardScaler` (very important for PCA).\n",
    "3.  **Part A: Find the best K**\n",
    "    * Fit a `sklearn.decomposition.PCA` model *without* setting `n_components` (so it fits all components).\n",
    "    * Plot the **cumulative explained variance** (using `np.cumsum(pca.explained_variance_ratio_)`).\n",
    "    * Add horizontal lines at `y=0.80` and `y=0.95` to see how many components are needed.\n",
    "4.  **Part B: Apply PCA and Visualize**\n",
    "    * Fit and transform the data using `PCA(n_components=2)`.\n",
    "    * Create a scatter plot of the resulting 2-dimensional data, colored by the *true person ID* (`faces.target`).\n",
    "    * Observe if the different people are separable in this 2D space.\n",
    "5.  **Part C: Visualize the \"Eigenfaces\"**\n",
    "    * From the `pca_full` model, get the first principal component (`.components_[0]`).\n",
    "    * Reshape it to `(64, 64)` and plot it using `plt.imshow` with `cmap='gray'`. This is one of the \"basis\" images, or \"eigenfaces,\" shown in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb8e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Load the dataset\n",
    "# This dataset contains 400 images of 40 different people (10 images each)\n",
    "# Each image is 64x64 pixels = 4096 dimensions\n",
    "# Note: This will download ~1.3MB of data the first time you run it.\n",
    "faces = fetch_olivetti_faces()\n",
    "X_pca_data = faces.data\n",
    "y_pca_data = faces.target\n",
    "\n",
    "print(f\"Original data shape: {X_pca_data.shape}\")\n",
    "\n",
    "# 2. Standardize the data\n",
    "scaler = ... # [TODO: Initialize StandardScaler]\n",
    "X_pca_scaled = ...# [TODO: Fit and transform the data]\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# --- 3. Part A: Find the best K ---\n",
    "plt.subplot(1, 3, 1)\n",
    "\n",
    "# Fit PCA to get all components\n",
    "pca_full = ... # [TODO: Initialize PCA() without setting n_components]\n",
    "pca_full.fit(...) # [TODO: Fit PCA to the scaled data]\n",
    "\n",
    "# Calculate cumulative variance\n",
    "cumulative_variance = ... # [TODO: Calculate the cumulative sum of pca_full.explained_variance_ratio_, and use np.cumsum()]\n",
    "\n",
    "# Plot\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='.')\n",
    "plt.title('8a. Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Components (K)')\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.grid(True)\n",
    "plt.axhline(0.95, color='red', linestyle='--', label='95% Variance')\n",
    "plt.axhline(0.80, color='orange', linestyle='--', label='80% Variance')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# --- 4. Part B: Apply PCA (K=2) and Visualize ---\n",
    "plt.subplot(1, 3, 2)\n",
    "\n",
    "# Fit and transform with K=2\n",
    "pca_2 = ... # [TODO: Initialize PCA with n_components=2]\n",
    "X_pca_2d = ... # [TODO: Fit PCA and transform the scaled data]\n",
    "\n",
    "print(f\"Reduced data shape (K=2): {X_pca_2d.shape}\")\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y_pca_data, cmap='viridis', alpha=0.7, s=20)\n",
    "plt.title('8b. Faces Dataset Visualized in 2D (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Person ID')\n",
    "plt.grid(True)\n",
    "\n",
    "# --- 5. Part C: Visualize the Eigenfaces ---\n",
    "plt.subplot(1, 3, 3)\n",
    "\n",
    "# The 'components' are the principal components, which are the \"eigenfaces\"\n",
    "# We reshape them back to 64x64 images to see what they \"learned\"\n",
    "eigenface_1 = ... # [TODO: Get the first component (index 0) from pca_full.components_ and reshape it to (64, 64)]\n",
    "\n",
    "plt.imshow(eigenface_1, cmap='gray')\n",
    "plt.title('8c. The First \"Eigenface\"\\n(Principal Component 1)')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfb087",
   "metadata": {},
   "source": [
    "### ðŸ”¬ Analysis 8\n",
    "\n",
    "[TODO: Write your analysis here... What does the variance plot (8a) tell you? How many components would *you* choose to keep? Looking at plot (8b), what does it mean that the clusters are mostly separate? Does this support the Manifold Hypothesis?, and look at plot (8c), the first \"eigenface\" / first principal component. Does it look like a real face? What features has it \"learned\" from the data? How does this relate to the \"basis\" images in the lecture slides?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e586d8d2",
   "metadata": {},
   "source": [
    "## How to submit\n",
    "#### First, choose File -> Print -> Save as PDF -> Save. Then, go to Gradescope and turn in the PDF to the corresponding assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23db7509",
   "metadata": {},
   "source": [
    "## Contributors\n",
    "- Patrick Mendoza"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
